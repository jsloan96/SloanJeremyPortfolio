	<!DOCTYPE html>
	<html lang="zxx" class="no-js">
	<head>
		<!-- Mobile Specific Meta -->
		<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
		<!-- Favicon-->
		<link rel="shortcut icon" href="img/fav.png">
		<!-- Author Meta -->
		<meta name="author" content="colorlib">
		<!-- Meta Description -->
		<meta name="description" content="">
		<!-- Meta Keyword -->
		<meta name="keywords" content="">
		<!-- meta character set -->
		<meta charset="UTF-8">
		<!-- Site Title -->
		<title>Credit Card Application Classification using Machine Learning Models (Python)</title>

		<link href="https://fonts.googleapis.com/css?family=Poppins:100,200,400,300,500,600,700" rel="stylesheet"> 
			<!--
			CSS
			============================================= -->
			<link rel="stylesheet" href="css/linearicons.css">
			<link rel="stylesheet" href="css/font-awesome.min.css">
			<link rel="stylesheet" href="css/bootstrap.css">
			<link rel="stylesheet" href="css/magnific-popup.css">
			<link rel="stylesheet" href="css/jquery-ui.css">				
			<link rel="stylesheet" href="css/nice-select.css">							
			<link rel="stylesheet" href="css/animate.min.css">
			<link rel="stylesheet" href="css/owl.carousel.css">					
			<link rel="stylesheet" href="css/main.css">
		</head>
		<body>
			<header id="header">
				<div class="container main-menu">
					<div class="row align-items-center justify-content-between d-flex">
					  <div id="logo">
						<a href="index.html"><h3>J. Sloan </h3></a>
					  </div>
					  <nav id="nav-menu-container">
						<ul class="nav-menu">
						  <li><a href="index.html">Home</a></li>
						  <li><a href="about.html">About</a></li>
						  <li><a href="portfolio.html">Portfolio</a></li>
						  <li><a href="photography.html">Photography</a></li>          					          		          
						  <li><a href="contact.html">Contact</a></li>
						  <li><a href="SloanJeremyResume.pdf" class="genric-btn primary-border circle" target="_blank">Resume</a></li>
						</ul>
					  </nav><!-- #nav-menu-container -->		    		
					</div>
				</div>
			  </header><!-- #header -->
		  
			<!-- start banner Area -->
			<section class="relative about-banner">	
				<div class="overlay overlay-bg"></div>
				<div class="container">				
					<div class="row d-flex align-items-center justify-content-center">
						<div class="about-content col-lg-12">
							<h1 class="text-white">
								Project Details				
							</h1>	
							<p class="text-white link-nav"><a href="index.html">Home </a>  <span class="lnr lnr-arrow-right"></span><a href="portfolio.html">Portfolio </a> <span class="lnr lnr-arrow-right"></span> <a href="Credit_Card_Application_Classification_using_Machine_Learning_Models_(Python).html"> Credit Card Application Classification using Machine Learning Models (Python)</a></p>
						</div>	
					</div>
				</div>
			</section>
			<!-- End banner Area -->					  
			
			<!-- Start post-content Area -->
			<section class="post-content-area single-post-area">
				<div class="container">
					<div class="row">
						<div class="row align-items-center justify-content-between d-flex">
							<div class="single-post row">
								<div class="col-lg-12">
																	
								</div>
								<div class="col-lg-3  col-md-3 meta-details">
									<ul class="tags">
										<li><a href="#">Food,</a></li>
										<li><a href="#">Technology,</a></li>
										<li><a href="#">Politics,</a></li>
										<li><a href="#">Lifestyle</a></li>
										<li><a href="#">Technology,</a></li>
										<li><a href="#">Politics,</a></li>
										<li><a href="#">Lifestyle</a></li>
									</ul>
									<div class="user-details row">
										<p class="user-name col-lg-12 col-md-12 col-6"><a href="#">Jeremy Sloan</a> <span class="lnr lnr-user"></span></p>
										<p class="date col-lg-12 col-md-12 col-6"><a href="#">12 Dec, 2017</a> <span class="lnr lnr-calendar-full"></span></p>
										<p class="date col-lg-12 col-md-12 col-6"><a href="https://github.com/jsloan96/">View code here</a> <span class="fa fa-github"></span></p>																				
									</div>
								</div>
								<div class="col-lg-9 col-md-9">
									<h3 class="mt-20 mb-20">Credit Card Application Classification using Machine Learning Models (Python)</h3>
									<div class="feature-img">
										<img class="img-fluid" src="img/Classification-in-Machine-Learning.jpeg" alt="">
									</div>	
									<!-- Start Introduction Section -->
									<h4 class="mt-20 mb-20">Intrduction</h4>
									<p class="excert">
										In the Machine Learning terminology, the process of Classification can be defined as a supervised learning algorithm that aims at categorizing 
										a set of data into different classes. In other words, if we think of a data set as a set of data instances, and each data instance as a set of features, 
										then Classification is the process of predicting the particular class that that individual data instance might belong to, based on its features.
										There are many applications in classification in many domains such as in credit approval, medical diagnosis, target marketing etc.
									</p>
									<p>
										For example, spam detection in email service providers can be identified as a classification problem. This is s binary classification since there are 
										only 2 classes as spam and not spam. A classifier utilizes some training data to understand how given input variables relate to the class. In this case, 
										known spam and non-spam emails have to be used as the training data. When the classifier is trained accurately, it can be used to detect an unknown email.
									</p>
									<p>
										Unlike regression where the target variable (i.e., the predicted value) belongs to a continuous distribution, in case of classification, the target variable 
										is discrete. It can only be one of the various target classes in a given problem.
									</p>
									<p>
										This project will use the Drug Classification data set available on Kaggle to demonstrate 7 different machine learning categorical models:
									</p>
									<!-- Start Two Column List Area -->
									<div class="row mt-30 mb-30">
										<div class="col-6">
									<ul style="list-style-type:disc;">
										<li>Logistic Regression</li>
										<li>K-Nearest Neighbor</li>
										<li>Support Vector Machine (SVM)</li>
										<li>1 Categorical Naive Bayes</li>
									  </ul>
									</div>
									<div class="col-6">
										<ul style="list-style-type:disc;">
											<li>2 Gaussian Naive Bayes</li>
											<li>Decision Tree</li>
											<li>Random Forest</li>
										</ul>
									</div>
								</div>
								<!-- End Two Column List Area -->
									<p>
										Because the Drug Classification data set contains information on 4 different drugs, we will be using multi-label Classification. Multi-Label Classification â€“ 
										As the name suggests, the prediction for a data instance can be more than 2 possible discrete outcomes. For example, when predicting the correct drug a patient 
										is taking, a given data instance representing a patient can belong to any one of the 4 classes.The target feature of the data set is Drug type and the feature 
										sets are Age, Sex, Blood Pressure, Cholesterol Levels, and Na to Potassium Levels. This project will focus on the 7 classification models listed above, exploratory
										data analysis and data visualizations can be also be found in the project files <a href="https://github.com/jsloan96/Classification_Project" target="_blank"> here.</a>. 
									</p>
									<!-- End Introduction Section -->

									<!-- Start Prepare the Data for Modeling Section -->
									<h4 class="mt-20 mb-20">Prepare the Data for Modeling</h4>
									<!-- Start Data Bining Sub-Section -->
									<h5 class="mt-20 mb-20">Data Bining</h5>
									<p>
										Data binning, is a data pre-processing method used to minimize the effects of small observation errors. The original data values are divided into small intervals known 
										as bins and then they are replaced by a general value calculated for that bin. This has a smoothing effect on the input data and may also reduce the chances of over-fitting 
										in the case of small datasets. We have 2 variables that require binning:
									</p>
									<h6 class="mt-20 mb-20">Age:</h6>
									<ul style="list-style-type:disc;">
										<li>Below 20 y.o.</li>
										<li>20 - 29 y.o.</li>
										<li>30 - 39 y.o.</li>
										<li>40 - 49 y.o.</li>
										<li>50 - 59 y.o.</li>
										<li>60 - 69 y.o.</li>
										<li>Above 70</li>
									  </ul> 
									  <div class="quotes">
										<code>bin_age = [0, 19, 29, 39, 49, 59, 69, 80]<br>
											category_age = ['<20s', '20s', '30s', '40s', '50s', '60s', '>60s']<br>
											drug_df['Age_binned'] = pd.cut(drug_df['Age'], bins=bin_age, labels=category_age)<br>
											drug_df = drug_df.drop(['Age'], axis = 1)</code>										
									</div>
									<h6 class="mt-20 mb-20">Na_to_K:</h6>
									<ul style="list-style-type:disc;">
										<li>Below 10</li>
										<li>10 - 20</li>
										<li>20 - 30</li>
										<li>Above 30</li>
									  </ul> 
									  <div class="quotes">
										<code>bin_NatoK = [0, 9, 19, 29, 50]<br>
											category_NatoK = ['<10', '10-20', '20-30', '>30']<br>
											drug_df['Na_to_K_binned'] = pd.cut(drug_df['Na_to_K'], bins=bin_NatoK, labels=category_NatoK)<br>
											drug_df = drug_df.drop(['Na_to_K'], axis = 1)</code>										
									</div>
									<!-- End Data Bining Sub-Section -->

									<!-- Start Splitting the Data Set Sub-Section -->
									<h5 class="mt-20 mb-20">Splitting the Data Set</h5>
									<p>
										We will be using the SciKitLearn package to split our data and build our models.
									</p>
									<div class="quotes">
										<code>from sklearn.model_selection import train_test_split<br>
											from sklearn.metrics import classification_report<br>
											from sklearn.metrics import confusion_matrix</code>										
									</div>
									<p>
										We first need to separate the response variable from the predictor variables before spiting the training and test samples. 
										Because we are using a small data set, to maximize the accuracy of our models we will split 33% test sample and 67% training sample.
									</p>
									<div class="quotes">
										<code>X = drug_df.drop(["Drug"], axis=1)<br>
											y = drug_df["Drug"]<br>
											x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)</code>										
									</div>
									<!-- Start Two Column Bullet List -->
									<div class="col-6">
										<img class="img-fluid" src="img/Classification-in-Machine-Learning.jpeg" alt="">
									</div>

									<div class="row mt-30 mb-30">
										<div class="col-6">
											<h6 class="mt-20 mb-20">Na_to_K:</h6>
									<ul style="list-style-type:disc;">
										<li>Below 10</li>
										<li>10 - 20</li>
										<li>20 - 30</li>
										<li>Above 30</li>
									  </ul> 
										</div>
										<div class="col-6">
											<h6 class="mt-20 mb-20">Na_to_K:</h6>
											<ul style="list-style-type:disc;">
												<li>Below 10</li>
												<li>10 - 20</li>
												<li>20 - 30</li>
												<li>Above 30</li>
											</ul> 
										</div>
										<!-- End Two Column Bullet List -->

										<!-- End Splitting the Data Set Sub-Section -->

										<!-- Start Balance the Data Set using the SMOTE Technique Sub-Section -->
										<h5 class="mt-20 mb-20">Balance the Data Set using the SMOTE Technique</h5>
									<p>
										SMOTE or Synthetic Minority Oversampling Technique is an oversampling technique but SMOTE working differently than your typical oversampling.
									</p>
									<p>
										In a classic oversampling technique, the minority data is duplicated from the minority data population. While it increases the number of data, 
										it does not give any new information or variation to the machine learning model.
									</p>
									<p>
										SMOTE works by utilizing a k-nearest neighbor algorithm to create synthetic data. SMOTE first start by choosing random data from the minority class, 
										then k-nearest neighbors from the data are set. Synthetic data would then be made between the random data and the randomly selected k-nearest neighbor. 
									</p>
									<p>
										Balancing the data using the SMOTE technique can be accomplished by using importing the SMOTE library from the imblearn package.
									</p>
									<div class="quotes">
										<code>from imblearn.over_sampling import SMOTE<br>
											x_train, y_train = SMOTE().fit_resample(x_train, y_train)</code>										
									</div>
									<div class="row mt-30 mb-30">
										<div class="col-6">
											<img class="img-fluid" src="img/Classification-in-Machine-Learning.jpeg" alt="">
										</div>
										<div class="col-6">
											<img class="img-fluid" src="img/Classification-in-Machine-Learning.jpeg" alt="">
										</div>
										<div class="col-lg-12 mt-30">
											<p>
												As you can see drug A, B, C, and X have had synthetic data added to them so that there are the same number of 'patients' taking each drug as drug Y.
											</p>
											<!-- End Balance the Data Set using the SMOTE Technique Sub-Section -->

											<!-- End Prepare the Data for Modeling Section -->
											
											<!-- Start Modeling Section -->
											<h4 class="mt-20 mb-20">Modeling</h4>
											<!-- Start Logistic Regressiion Sub-Section -->
											<h5 class="mt-20 mb-20">Logistic Regression</h5>
											<p>
												This type of statistical model (also known as logit model) is often used for classification and predictive analytics. Logistic regression estimates the probability 
												of an event occurring, such as voted or didnâ€™t vote, based on a given data set of independent variables. Since the outcome is a probability, the dependent variable 
												is bounded between 0 and 1. In logistic regression, a logit transformation is applied on the oddsâ€”that is, the probability of success divided by the probability of 
												failure. This is also commonly known as the log odds, or the natural logarithm of odds.
											</p>
											<div class="quotes">
												<code>from sklearn.linear_model import LogisticRegression<br>

													LRclassifier = LogisticRegression(solver='liblinear', max_iter=5000)<br>
													LRclassifier.fit(x_train, y_train)<br>
													
													y_pred = LRclassifier.predict(x_test)<br>
													
													print(classification_report(y_test, y_pred))<br>
													print(confusion_matrix(y_test, y_pred))<br>
													
													from sklearn.metrics import accuracy_score<br>
													
													LRAcc = accuracy_score(y_pred,y_test)<br>
													print('Logistic Regression accuracy is: {:.2f}%'.format(LRAcc*100))<br>
												</code>										
											</div>
											<div class="col-6">
												<img class="img-fluid" src="img/Classification-in-Machine-Learning.jpeg" alt="">
											</div>
		
											<div class="row mt-30 mb-30">
												<div class="col-6">
													<h6 class="mt-20 mb-20">Advantages:</h6>
											<ul style="list-style-type:disc;">
												<li>Logistic regression works well when the data is linearly separable, i.e., if all the data instances are plotted on a scatter plot, there must be a line that divides 
													the data in such a way such that data instances belonging to the same class end up together on the same side of the line.</li>
												<li>Logistic regression is somewhat less prone to over-fitting, especially when the dimensionality of the data is low.</li>
												<li>It is one of the most easy-to-implement classification algorithms.</li>
											  </ul> 
												</div>
												<div class="col-6">
													<h6 class="mt-20 mb-20">Disadvantages:</h6>
													<ul style="list-style-type:disc;">
														<li>As the dimensionality of data increases, i.e., the number of features used to train the classifier increases, the chances of over-fitting increases.</li>
														<li>Logistic regression assumes that the data that is being used is linearly separable. In real-world problems however, the data might contain a lot of outliers. 
															Thus, logistic regression is very sensitive to outliers, and is overall a bad choice as a classification algorithm if your data is not linearly separable.</li>
													</ul> 
												</div>
												<!-- End Logistic Regressiion Sub-Section -->
												<!-- Start K-Nearest Neighbors Sub-Section -->
											<h5 class="mt-20 mb-20">K-Nearest Neighbor (KNN)</h5>
											<p>
												K-Nearest Neighbor is a lazy learning algorithm which stores all instances correspond to training data points in n-dimensional space. When an unknown discrete data is received, 
												it analyzes the closest k number of instances saved (nearest neighbors)and returns the most common class as the prediction and for real-valued data it returns the mean of k nearest neighbors.
											</p>
											<p>
												In the distance-weighted nearest neighbor algorithm, it weights the contribution of each of the k neighbors according to their distance using the following query giving greater weight to the closest neighbors.
												Usually KNN is robust to noisy data since it is averaging the k-nearest neighbors.
											</p>
											<div class="quotes">
												<code>from sklearn.neighbors import KNeighborsClassifier<br>

													KNclassifier = KNeighborsClassifier(n_neighbors=20)<br>
													KNclassifier.fit(x_train, y_train)<br>
													
													y_pred = KNclassifier.predict(x_test)<br>
													
													print(classification_report(y_test, y_pred))<br>
													print(confusion_matrix(y_test, y_pred))<br>
													
													KNAcc = accuracy_score(y_pred,y_test)<br>
													print('K Neighbours accuracy is: {:.2f}%'.format(KNAcc*100))<br>
												</code>										
											</div>
											<div class="col-6">
												<img class="img-fluid" src="img/Classification-in-Machine-Learning.jpeg" alt="">
											</div>
		
											<div class="row mt-30 mb-30">
												<div class="col-6">
													<h6 class="mt-20 mb-20">Advantages:</h6>
											<ul style="list-style-type:disc;">
												<li>Since KNN is a non-parametric algorithm, it doesnâ€™t require several training cycles in order to optimize its parameters. As a result, it has one of the shortest training 
													times. This makes it an ideal choice for batch training systems, where we have to train a new model each time in order to update it for the latest data.</li>
												<li>It is very easy to implement and understand.</li>
												<li>KNN algorithm works for both binary as well as multi-class classification problems.</li>
											  </ul> 
												</div>
												<div class="col-6">
													<h6 class="mt-20 mb-20">Disadvantages:</h6>
													<ul style="list-style-type:disc;">
														<li>The inference can be compute-intensive. This is because in order to find the k-nearest training data points for the given new data, the distance from each training 
															instance has to be calculated every single time.</li>
														<li>The accuracy is inversely proportional to the dimensionality of the data set. The higher the dimensionality, the more are the chances that the accuracy of the model will decrease.</li>
														<li>In order to get the optimal performance, you need to find the best value for the hyper-parameter â€˜kâ€™ that determines the number of neighbors to consider in order make the predictions. 
															Hyper-parameter optimization (i.e., choosing a set of best hyper-parameter values for which the model gives an optimum performance)can be a tricky task and requires a lot of experimentation.</li>
													</ul> 
												</div>
												<!-- End K-Nearest Neighbor Sub-Section -->
												<!-- Start Support Vector Machine (SVM) Sub-Section -->
											<h5 class="mt-20 mb-20">Support Vector Machine (SVM)</h5>
											<p>
												Support Vector Machine, abbreviated as SVM can be used for both regression and classification tasks. But, it is widely used in classification objectives. The objective of 
												the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N â€” the number of features) that distinctly classifies the data points.
											</p>
											<p>
												To separate the two classes of data points, there are many possible hyper-planes that could be chosen. Our objective is to find a plane that has the maximum margin, i.e the 
												maximum distance between data points of both classes. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence.
											</p>
											<div class="quotes">
												<code>from sklearn.svm import SVC<br>

													SVCclassifier = SVC(kernel='linear', max_iter=251)<br>
													SVCclassifier.fit(x_train, y_train)<br>
													
													y_pred = SVCclassifier.predict(x_test)<br>
													
													print(classification_report(y_test, y_pred))<br>
													print(confusion_matrix(y_test, y_pred))<br>
													
													SVCAcc = accuracy_score(y_pred,y_test)<br>
													print('SVC accuracy is: {:.2f}%'.format(SVCAcc*100))<br>
												</code>										
											</div>
											<div class="col-6">
												<img class="img-fluid" src="img/Classification-in-Machine-Learning.jpeg" alt="">
											</div>
		
											<div class="row mt-30 mb-30">
												<div class="col-6">
													<h6 class="mt-20 mb-20">Advantages:</h6>
											<ul style="list-style-type:disc;">
												<li>The inference on SVM model is very fast.</li>
												<li>SVM works well with high dimensional data.</li>
											  </ul> 
												</div>
												<div class="col-6">
													<h6 class="mt-20 mb-20">Disadvantages:</h6>
													<ul style="list-style-type:disc;">
														<li>As the scale of the data (i.e., the number of data instances in the data set) increases, it becomes tougher to clearly differentiate the two classes via a hyperplane. 
															As a result, the performance starts to deteriorate as the size of the dataset increases.</li>
														<li>SVM algorithm has a very bad response to outliers. Therefore, it results in really bad accuracy if the training data has a lot of outliers.</li>
														<li>Good for binary classification, but the complexity of the model increases exponentially as the number of target classes increases.</li>
													</ul> 
												</div>
												<!-- End Support Vector Machine (SVM) Sub-Section -->
												<!-- Start Naive Bayes Sub-Section -->
											<h5 class="mt-20 mb-20">Naive Bayes</h5>
											<p>
												Naive Bayes is a probabilistic classifier inspired by the Bayes theorem under a simple assumption which is the attributes are conditionally independent.The classification is conducted 
												by deriving the maximum posterior which is the maximal P(Ci|X) with the above assumption applying to Bayes theorem. This assumption greatly reduces the computational cost by only counting 
												the class distribution. Even though the assumption is not valid in most cases since the attributes are dependent, surprisingly Naive Bayes has able to perform impressively.
											</p>
											<p>
												Naive Bayes is a very simple algorithm to implement and good results have obtained in most cases. It can be easily scalable to larger datasets since it takes linear time, rather than by 
												expensive iterative approximation as used for many other types of classifiers. Naive Bayes can suffer from a problem called the zero probability problem. When the conditional probability 
												is zero for a particular attribute, it fails to give a valid prediction. This needs to be fixed explicitly using a Laplacian estimator.
											</p>
										</p>
										<div class="quotes">
											<code>from sklearn.naive_bayes import CategoricalNB<br>
												<br>
												NBclassifier1 = CategoricalNB()<br>
												NBclassifier1.fit(x_train, y_train)<br>
												<br>
												y_pred = NBclassifier1.predict(x_test)<br>
												<br>
												print(classification_report(y_test, y_pred))<br>
												print(confusion_matrix(y_test, y_pred))<br>
												<br>
												NBAcc1 = accuracy_score(y_pred,y_test)<br>
												print('Naive Bayes accuracy is: {:.2f}%'.format(NBAcc1*100))<br>
											</code>										
										</div>
										<div class="col-6">
											<img class="img-fluid" src="img/Classification-in-Machine-Learning.jpeg" alt="">
										</div>
									</p>
									<div class="quotes">
										<code>from sklearn.naive_bayes import GaussianNB<br>
											<br>
											NBclassifier2 = GaussianNB()<br>
											NBclassifier2.fit(x_train, y_train)<br>
											<br>
											y_pred = NBclassifier2.predict(x_test)<br>
											<br>
											print(classification_report(y_test, y_pred))<br>
											print(confusion_matrix(y_test, y_pred))<br>
											<br>
											NBAcc2 = accuracy_score(y_pred,y_test)<br>
											print('Gaussian Naive Bayes accuracy is: {:.2f}%'.format(NBAcc2*100))<br>
										</code>										
									</div>
									</div>
											<div class="col-6">
												<img class="img-fluid" src="img/Classification-in-Machine-Learning.jpeg" alt="">
											</div>
		
											<div class="row mt-30 mb-30">
												<div class="col-6">
													<h6 class="mt-20 mb-20">Advantages:</h6>
											<ul style="list-style-type:disc;">
												<li>Works well even if the scale of the data is very large. Hence, it has a high scalability as compared to many other classifiers.</li>
												<li>Is robust to outliers, hence gives good accuracy even if the data set has a lot of outliers. </li>
												<li>Works well with both continuous and discrete data, and hence is very flexible.</li>
											  </ul> 
												</div>
												<div class="col-6">
													<h6 class="mt-20 mb-20">Disadvantages:</h6>
													<ul style="list-style-type:disc;">
														<li>The model assumes that each feature in the data set is independent. However, in real-world problems, the features in a data set are often dependent on each other.</li>
													</ul> 
												</div>
												<!-- End Naive Bayes Sub-Section -->
												<!-- Start Decision Tree Sub-Section -->
											<h5 class="mt-20 mb-20">K-Nearest Neighbor (KNN)</h5>
											<p>
												K-Nearest Neighbor is a lazy learning algorithm which stores all instances correspond to training data points in n-dimensional space. When an unknown discrete data is received, 
												it analyzes the closest k number of instances saved (nearest neighbors)and returns the most common class as the prediction and for real-valued data it returns the mean of k nearest neighbors.
											</p>
											<p>
												In the distance-weighted nearest neighbor algorithm, it weights the contribution of each of the k neighbors according to their distance using the following query giving greater weight to the closest neighbors.
												Usually KNN is robust to noisy data since it is averaging the k-nearest neighbors.
											</p>
											<div class="quotes">
												<code>from sklearn.neighbors import KNeighborsClassifier<br>

													KNclassifier = KNeighborsClassifier(n_neighbors=20)<br>
													KNclassifier.fit(x_train, y_train)<br>
													
													y_pred = KNclassifier.predict(x_test)<br>
													
													print(classification_report(y_test, y_pred))<br>
													print(confusion_matrix(y_test, y_pred))<br>
													
													KNAcc = accuracy_score(y_pred,y_test)<br>
													print('K Neighbours accuracy is: {:.2f}%'.format(KNAcc*100))<br>
												</code>										
											</div>
											<div class="col-6">
												<img class="img-fluid" src="img/Classification-in-Machine-Learning.jpeg" alt="">
											</div>
		
											<div class="row mt-30 mb-30">
												<div class="col-6">
													<h6 class="mt-20 mb-20">Advantages:</h6>
											<ul style="list-style-type:disc;">
												<li>Since KNN is a non-parametric algorithm, it doesnâ€™t require several training cycles in order to optimize its parameters. As a result, it has one of the shortest training 
													times. This makes it an ideal choice for batch training systems, where we have to train a new model each time in order to update it for the latest data.</li>
												<li>It is very easy to implement and understand.</li>
												<li>KNN algorithm works for both binary as well as multi-class classification problems.</li>
											  </ul> 
												</div>
												<div class="col-6">
													<h6 class="mt-20 mb-20">Disadvantages:</h6>
													<ul style="list-style-type:disc;">
														<li>The inference can be compute-intensive. This is because in order to find the k-nearest training data points for the given new data, the distance from each training 
															instance has to be calculated every single time.</li>
														<li>The accuracy is inversely proportional to the dimensionality of the data set. The higher the dimensionality, the more are the chances that the accuracy of the model will decrease.</li>
														<li>In order to get the optimal performance, you need to find the best value for the hyper-parameter â€˜kâ€™ that determines the number of neighbors to consider in order make the predictions. 
															Hyper-parameter optimization (i.e., choosing a set of best hyper-parameter values for which the model gives an optimum performance)can be a tricky task and requires a lot of experimentation.</li>
													</ul> 
												</div>
												<!-- End Decision Tree Sub-Section -->
												<!-- Start Random Forest Sub-Section -->
											<h5 class="mt-20 mb-20">K-Nearest Neighbor (KNN)</h5>
											<p>
												K-Nearest Neighbor is a lazy learning algorithm which stores all instances correspond to training data points in n-dimensional space. When an unknown discrete data is received, 
												it analyzes the closest k number of instances saved (nearest neighbors)and returns the most common class as the prediction and for real-valued data it returns the mean of k nearest neighbors.
											</p>
											<p>
												In the distance-weighted nearest neighbor algorithm, it weights the contribution of each of the k neighbors according to their distance using the following query giving greater weight to the closest neighbors.
												Usually KNN is robust to noisy data since it is averaging the k-nearest neighbors.
											</p>
											<div class="quotes">
												<code>from sklearn.neighbors import KNeighborsClassifier<br>

													KNclassifier = KNeighborsClassifier(n_neighbors=20)<br>
													KNclassifier.fit(x_train, y_train)<br>
													
													y_pred = KNclassifier.predict(x_test)<br>
													
													print(classification_report(y_test, y_pred))<br>
													print(confusion_matrix(y_test, y_pred))<br>
													
													KNAcc = accuracy_score(y_pred,y_test)<br>
													print('K Neighbours accuracy is: {:.2f}%'.format(KNAcc*100))<br>
												</code>										
											</div>
											<div class="col-6">
												<img class="img-fluid" src="img/Classification-in-Machine-Learning.jpeg" alt="">
											</div>
		
											<div class="row mt-30 mb-30">
												<div class="col-6">
													<h6 class="mt-20 mb-20">Advantages:</h6>
											<ul style="list-style-type:disc;">
												<li>Since KNN is a non-parametric algorithm, it doesnâ€™t require several training cycles in order to optimize its parameters. As a result, it has one of the shortest training 
													times. This makes it an ideal choice for batch training systems, where we have to train a new model each time in order to update it for the latest data.</li>
												<li>It is very easy to implement and understand.</li>
												<li>KNN algorithm works for both binary as well as multi-class classification problems.</li>
											  </ul> 
												</div>
												<div class="col-6">
													<h6 class="mt-20 mb-20">Disadvantages:</h6>
													<ul style="list-style-type:disc;">
														<li>The inference can be compute-intensive. This is because in order to find the k-nearest training data points for the given new data, the distance from each training 
															instance has to be calculated every single time.</li>
														<li>The accuracy is inversely proportional to the dimensionality of the data set. The higher the dimensionality, the more are the chances that the accuracy of the model will decrease.</li>
														<li>In order to get the optimal performance, you need to find the best value for the hyper-parameter â€˜kâ€™ that determines the number of neighbors to consider in order make the predictions. 
															Hyper-parameter optimization (i.e., choosing a set of best hyper-parameter values for which the model gives an optimum performance)can be a tricky task and requires a lot of experimentation.</li>
													</ul> 
												</div>
												<!-- End Random Forest Sub-Section -->
												<!-- End Modeling Section -->
											</div>
										</div>
									</div>
								</div>
							</section>
							<!-- End post-content Area -->
			
            <!-- start footer Area -->
            <footer class="footer-area section-gap">
                <div class="container">
                    <div class="row">
                        <div class="col-lg-5 col-md-6 col-sm-6">
                            <div class="single-footer-widget">
                                <h4>About This Page</h4>
                                <p>
                                    This website was coded in HTML, CSS, and Javascript based on a theme from Colorlib. It is hosted on a private server from DigitalOcean. The code can be found <a href="https://github.com/jsloan96/SloanJeremyPortfolio" target="_blank"> here.</a>
                                </p>
                                <p class="footer-text"><!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
Copyright &copy;<script>document.write(new Date().getFullYear());</script> All rights reserved | This template is made with <i class="fa fa-heart-o" aria-hidden="true"></i> by <a href="https://colorlib.com" target="_blank">Colorlib</a>
<!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. --></p>
                            </div>
                        </div>
                        <div class="col-lg-5 col-md-6 col-sm-6">
                            <div class="single-footer-widget">
                                <h4>Newsletter</h4>
                                <p>Stay updated with my latest projects</p>
								<div class="" id="mc_embed_signup">
									 <form target="_blank" action="https://spondonit.us12.list-manage.com/subscribe/post?u=1462626880ade1ac87bd9c93a&amp;id=92a4423d01" method="get">
									  <div class="input-group">
									    <input type="text" class="form-control" name="EMAIL" placeholder="Enter Email Address" onfocus="this.placeholder = ''" onblur="this.placeholder = 'Enter Email Address '" required="" type="email">
									    <div class="input-group-btn">
									      <button class="btn btn-default" type="submit">
									        <span class="lnr lnr-arrow-right"></span>
									      </button>    
									    </div>
									    	<div class="info"></div>  
									  </div>
									</form> 
								</div>
                            </div>
                        </div>
                        <div class="col-lg-2 col-md-6 col-sm-6 social-widget">
                            <div class="single-footer-widget">
                                <h4>Follow Me</h4>
                                <p>Let's be social</p>
                                <div class="footer-social d-flex align-items-center">
                                    <a href="http://www.linkedin.com/in/jeremy-sloan96"><i class="fa fa-linkedin"></i></a>
									<a href="https://github.com/jsloan96"><i class="fa fa-github"></i></a>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </footer>
            <!-- End footer Area -->			

			<script src="js/vendor/jquery-2.2.4.min.js"></script>
			<script src="js/popper.min.js"></script>
			<script src="js/vendor/bootstrap.min.js"></script>			
			<script src="https://maps.googleapis.com/maps/api/js?key=AIzaSyBhOdIF3Y9382fqJYt5I_sswSrEw5eihAA"></script>			
  			<script src="js/easing.min.js"></script>			
			<script src="js/hoverIntent.js"></script>
			<script src="js/superfish.min.js"></script>	
			<script src="js/jquery.ajaxchimp.min.js"></script>
			<script src="js/jquery.magnific-popup.min.js"></script>	
    		<script src="js/jquery.tabs.min.js"></script>						
			<script src="js/jquery.nice-select.min.js"></script>	
            <script src="js/isotope.pkgd.min.js"></script>			
			<script src="js/waypoints.min.js"></script>
			<script src="js/jquery.counterup.min.js"></script>
			<script src="js/simple-skillbar.js"></script>							
			<script src="js/owl.carousel.min.js"></script>							
			<script src="js/mail-script.js"></script>	
			<script src="js/main.js"></script>	
		</body>
	</html>